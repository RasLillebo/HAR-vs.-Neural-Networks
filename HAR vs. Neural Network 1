#Load Packages
library(neuralnet)
library(HARModel)
library(Hmisc)
library(readr)
library(dplyr)
library(nnet)

#Load Data (Index, Rvol, Date)
# "Rvol" is the realized volatility of 20 DOW Jones stocks. The data was extracted by bivariate 
#  analysis on 2 years of data at the millisecond level, and the re-specified to 5-minute level. (See: Sheppard, Patton, Liu,. 2012)
Data <-  read_csv("C:/Users/PC/Downloads/Data.csv")
MyData = Data$Rvol
NNData = Data %>% mutate(NN1 = (MyData-Lag(MyData, -1))/2,      
                           NN5 = (MyData-Lag(MyData, -5))/5,              
                           NN22 = (MyData-Lag(MyData, -22))/22)           
NNData = data.frame(NNData[c(-1, -3)])                   
NNData = na.exclude(NNData)                  

#Checking data to see if it looks correct. Higher lags should be more stationary.
(ggplot(NNData,aes(Data$Date[1:463], y=value,color=variable)) + geom_point(aes(y=NNData$NN1,col="L1")) 
                                                             + geom_point(aes(y=NNData$NN5,col="L5")) 
                                                             + geom_point(aes(y=NNData$NN22,col="L22")))

# Missing Data?
apply(NNData,2,function(x) sum(is.nan(x))) #Checking for 'Not A Number'
apply(NNData,2,function(x) sum(is.na(x)))  #Checking for 'Not Announced'

# Split data in 25% test 75% training
index <- sample(1:nrow(NNData),round(0.75*nrow(NNData)))

train <- NNData[index,]                      #Training set
test <- NNData[-index,]                      #Test set

#Har model
HAR.fit <- glm(Rvol~ NN1 + NN5 + NN22, data=train) #HAR model
summary(HAR.fit)                             #Summarize Model
pr.HAR <- predict(HAR.fit,test)              #Predicting HAR model
MSE.HAR <- sum((pr.HAR - test$Rvol)^2)/nrow(test) #Calculating MSE

#Neural Network
maxs <- apply(NNData,2, max)  #Normalizing data (max)
mins <- apply(NNData, 2, min) #Normalizing data (min)
scaled <- as.data.frame(scale(NNData, center = mins, scale = maxs - mins))  #Scaling the data into Max and Min
train_ <- scaled[index,]     #Creating training set from scaled data
test_ <- scaled[-index,]    #Creating test set from scaled data

# Setup the model
n <- names(train_)          #Applying reference to scaled set
f <- as.formula(paste("Rvol ~", paste(n[!n %in% "Rvol"], collapse = " + "))) #Applying reference to formula
nn <- neuralnet(f,data=train_,hidden=c(4,2),linear.output=T)  #Neural Network
plot(nn)                   #Plotting Neural Network

# Predict median value
pr.nn <- neuralnet::compute(nn,test_, rep=1)    #Predicting using 'compute' from nnet-package
pr.nn_ <- pr.nn$net.result*(max(NNData$Rvol)-min(NNData$Rvol))+min(NNData$Rvol) #Scaling predictions
test.r <- (test_$Rvol)*(max(NNData$Rvol)-min(NNData$Rvol))+min(NNData$Rvol)       #Creating scaled test set for the MSE
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)     #Calculating MSE

# Compare MSE's
print(paste(MSE.HAR,MSE.nn))   #Printing MSE's

# Single plot
par(mfrow=c(1,1))    #Specifying window for plot
TrueValue = test$Rvol  #Renaming test set variable for plot (Could also be specified in 'legend')
Predicted = pr.nn_   #Renaming prediction for plot (Could also be specified in 'legend')
plot(TrueValue,Predicted,col='red',main='Predicted NN & HAR',pch=18,cex=0.7) #Plotting prediction with test set (NN).
points(test$Rvol,pr.HAR,col='blue',pch=18,cex=0.7)                            #Plotting prediction with test set (HAR)
abline(0,1,lwd=2)         #Specifying abline
legend('bottomright',legend=c('NN','HAR'),pch=18,col=c('red','blue')) #Specifying graphical add ons

# Cross-validation. Note: Starting with Neural Network
#NN
par(mfrow=c(2, 1))  #Specifying area for boxplot
library(boot)
library(plyr)
cv.errorNN <- NULL
pbar <- create_progress_bar('text')   #Creating process bar as seen in 'machine learning' class
k <- 10  #Specifying amount of folds
pbar$init(k)   #Specifying steps in process bar
for(i in 1:k){  #Looping over validation where k=10
  index <- sample(1:nrow(NNData),round(0.8*nrow(NNData)))   #Indexing and scaling as seen before:
  train.cv <- scaled[index,]
  test.cv <- scaled[-index,]
  nn <- neuralnet(f,data=train.cv,hidden=c(4,2),linear.output=T) 
  pr.nn <- neuralnet::compute(nn,test.cv[,1:4])
  pr.nn <- pr.nn$net.result*(max(NNData$Rvol)-min(NNData$Rvol))+min(NNData$Rvol)
  test.cv.r <- (test.cv$Rvol)*(max(NNData$Rvol)-min(NNData$Rvol))+min(NNData$Rvol)
  cv.errorNN[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv) #Collecting errors in variable
  pbar$step()  #Notify process bar of step
}
mean(cv.errorNN)  #Taking the MSE
cv.errorNN        #Looking at the different errors
boxplot(cv.errorNN,xlab='MSE CV',col='red', #Plotting the boxplot
        border='blue',names='CV error (MSE)',
        main='CV error (MSE) for NN',horizontal=TRUE)

#Har: Same steps as above
cv.errorHAR <- NULL
for(i in 1:k){
  cv.index <- sample(1:nrow(NNData),round(0.8*nrow(NNData)))
  train.cv <- scaled[index,]
  test.cv <- scaled[-index,]
  HAR.fit <-  glm(Rvol~ NN1 + NN5 + NN22,data=train.cv)
  cv.errorHAR[i] <-  cv.glm(train.cv,HAR.fit,K=10)$delta[1] #Collecting errors in variable
}
mean(cv.errorHAR) #Taking the MSE
cv.errorHAR       #Looking at different mean errors
boxplot(cv.errorHAR,xlab='MSE CV',col='lightblue',  #Plotting the boxplot
        border='blue',names='CV error (MSE)',
        main='CV error (MSE) for HAR',horizontal=TRUE)

########################
#Forecasting
########################
library(forecast)
par(mfrow=c(3, 1))  #Preparing window for plots
plot(test_$Rvol, type="l", main = "Realized Volatility") #Plot simple test set variable
#Neural Network
pr.nn_t <- neuralnet::compute(nn, test_)             #Predicting neural network
pr.nn_ <- as.numeric(pr.nn_t$net.result)  #Specifying format of prediction variable
NNFor <- forecast(pr.nn_,  h=5)           #Forecasting predictioin
plot(NNFor, col="red", main = "Neural Network") #Plotting forecast
accuracy(NNFor)                           #Computing accuracy for forecast
sumNN <- summary(NNFor)                   #Summarizing forecast
#HAR: Same as above. Note: Using 'predict' instead of 'compute'
pr.HAR_ <-  predict(HAR.fit, test_)
pr.HAR_ <- as.numeric(pr.HAR_)
HARFor <- forecast(pr.HAR_, h=5)
plot(HARFor, col="blue", main = "HAR")
sumHAR <- summary(HARFor)
accuracy(HARFor)

########################
#Bagging
########################
#Neural Networks
par(mfrow=c(3, 1))        #Preparing window for plots
plot(test_$Rvol, type="l", main = "Realized Volatility") #Plot simple test set variable
y = as.numeric(pr.nn_)             #Specifying format of variable
pr.NNBag = baggedModel(y, bootstrapped_series= bld.mbb.bootstrap(y, 100)) #Bagging over 100 sets
accuracy(pr.NNBag)                            #Computing accuracy of bagging predictions
NNBag = forecast(pr.NNBag, h = 5)             #Forecasting bagged prediction
plot(NNBag, col="red", main = "Bagged Neural Network")  #Plotting forecast of bagged prediction
summary(NNBag)                                #Summarizing forecast 

#HAR: Same as above
x = as.numeric(pr.HAR_)
pr.HARBag = baggedModel(x, bld.mbb.bootstrap(x, 100), fn=)
HARBag = forecast(pr.HARBag, h = 5)
plot(HARBag, col="blue", main = "Bagged HAR")

#Final Measurements
accuracy(NNBag)
accuracy((HARBag))



